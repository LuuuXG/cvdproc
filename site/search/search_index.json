{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CVDProc: CerebroVascular Disease imaging Processing","text":"<p>Warning</p> <p>This package is intended for research purposes only, to facilitate reproducibility of neuroimaging analyses in our center. The authors do NOT guarantee the correctness or clinical validity of all processing workflows.</p> <p>This repository currently serves as a public display for MRI image preprocessing and analysis, aimed at enhancing the transparency and reproducibility of research conducted at our center.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#download","title":"Download","text":"<p>You can download the source code by cloning the GitHub repository: <pre><code>git clone https://github.com/LuuuXG/cvdproc.git\n</code></pre> Or download it manually from the GitHub homepage.</p>"},{"location":"installation/#installation_1","title":"Installation","text":"<p>Please create a new conda environment, which is more suitable for this package.</p> <pre><code># Please replace &lt;env_name&gt; with the name you want for the environment\nconda create -n &lt;env_name&gt; python=3.7 openssl=1.1.1\nconda activate &lt;env_name&gt;\n</code></pre> <p>Python Version</p> <p>Due to the features of the early version of  SHIVA model, python 3.7 is used for tensorflow compatibility. We are working on updating the code to support higher versions of Python by using SHiVAi. For example, LST-AI for WMH segmentation needs python 3.8 or higher. Currently, the solution is to use a different environment for different pipelines if necessary.</p> <p>Then, navigate to the directory where you downloaded the code (the folder containing <code>setup.py</code>), and run the following command:</p> <pre><code># Use -e to allow modification of the code without needing to reinstall.\n# Here &lt;/path/to/cvdproc&gt; is the folder containing `setup.py`\npip install -e /path/to/cvdproc\n\n# If it is necessary to use mirror\npip install -e /path/to/cvdproc -i https://pypi.tuna.tsinghua.edu.cn/simple\n</code></pre> <p>tensorflow and torch</p> <p>Because these two packages are large, we do not specify them in <code>setup.py</code>. However, they will be used in the subsequent code. Please install them as needed.</p>"},{"location":"usage/","title":"Usage","text":"<p>Please refer to the different pages for specific functions:</p> <ul> <li>dcm2bids: Create BIDS dataset from DICOM files.</li> </ul> <p>Or if you already have a dataset in BIDS format, you can use the following functions:</p> <ul> <li>Pipelines: Different pipelines for MRI preprocessing and analysis.</li> </ul>"},{"location":"dcm2bids/dcm2bids/","title":"dcm2bids","text":"<p>We are using the dcm2bids to convert DICOM files to BIDS format. Before running the code, please make sure you have installed the <code>dcm2bids</code> (it should have been installed when you installed the cvdproc) and <code>dcm2niix</code> (you can install it via <code>apt install dcm2niix</code>)</p>"},{"location":"dcm2bids/dcm2bids/#create-a-new-bids-dataset","title":"Create a new BIDS dataset","text":"<p>If you want to create a new BIDS dataset, you can use the following command:</p> <pre><code>cvdproc --run_initialization &lt;path/to/the/folder/you/want/to/create&gt;\n</code></pre> <p>You don't need to create the folder manually, the code will create it for you.</p>"},{"location":"dcm2bids/dcm2bids/#convert-dicom-to-bids","title":"Convert DICOM to BIDS","text":"<p>If you already have a BIDS root folder or just created one with the command above, you can follow the steps below to convert DICOM files to BIDS format.</p>"},{"location":"dcm2bids/dcm2bids/#create-a-dcm2bids-configuration-file","title":"Create a dcm2bids configuration file","text":"<p>Please refer to the official dcm2bids documentation How to create a configuration file for the most detailed guidance.</p> <p>Here we take the example of converting a 3D T1w image to BIDS format.</p> <p>Create a file named <code>dcm2bids_config.json</code> in the <code>code</code> folder of your BIDS root directory (the file name and location can be changed, but we name it this way for convenience). The content of the file is as follows:</p> <pre><code>{\n  \"descriptions\": [\n    {\n      \"datatype\": \"anat\",\n      \"suffix\": \"T1w\",\n      \"criteria\": {\n        \"SeriesDescription\": \"*mprage*\",\n      }\n    }\n  ]\n}\n</code></pre> <p>The most important part of the configuration file is the <code>criteria</code> field, which specifies how to match the DICOM files. In this case, we are matching the <code>SeriesDescription</code> field with a regular expression <code>*mprage*</code>. If your DICOM files do not have this field or have a different value, you can try to use the <code>dcm2bids_helper</code> command to get the information, or you can use <code>dcm2niix</code> (which can also be found in MRIcroGL) to get the information in the JSON file. You can also skip this step (just copy the content above) and wait for the next step to see how we solve it.</p>"},{"location":"dcm2bids/dcm2bids/#create-a-cvdproc-configuration-file","title":"Create a cvdproc configuration file","text":"<p>cvdproc config file</p> <p>This step is very important because it is the core of the <code>cvdproc</code> command to specify parameters.</p> <p>Create a <code>config.yml</code> file in the <code>code</code> folder (note that we use the yaml format), and the content is as follows:</p> <pre><code># You need to specify the BIDS root folder here\nbids_dir: /mnt/f/BIDS/demo_wmh\n## You need to specify the dcm2bids configuration file here\ndcm2bids: /mnt/f/BIDS/demo_wmh/code/dcm2bids_config.json\n</code></pre> <p>Remember to change the <code>bids_dir</code> and <code>dcm2bids</code> paths to your own paths. The <code>bids_dir</code> is the root folder of your BIDS dataset, and the <code>dcm2bids</code> is the path to the dcm2bids configuration file you just created. And then you need to move or copy the folder containing the DICOM files for a single subject into the <code>sourcedata</code> folder of the BIDS root directory. For example, if you have a folder named <code>DICOM_01</code> containing the DICOM files for a single subject's baseline acquisition, you need to move or copy it to <code>/mnt/f/BIDS/demo_wmh/sourcedata/DICOM_01</code>. The final folder structure should look like this:</p> <p>Info</p> <p>Because the original DICOM images obtained in actual research may have different structures, the file structure under the subject's DICOM folder may vary. However, it should be noted that there is no need to preprocess the subfolders under the DICOM folder in advance (for example, a common practice is to make one subfolder correspond to one scanning sequence). This is because <code>dcm2bids</code> will convert all DICOM files found under the folder, even if only a few sequences are specified in the json file (so, for example, when EPI sequences with DWI or multi-echo GRE sequences are included, the conversion time may take several minutes, but fortunately, theoretically, such conversion only needs to be done once).</p> <p>After the above preparation, you need to specify the subject ID and session ID for the converted subject. For example, if the DICOM files are stored in <code>DICOM_01</code>, you need to set the subject ID to <code>SUB0001</code> and the session ID to <code>01</code> to indicate baseline data. Run:</p> <pre><code>cvdproc --config_file /mnt/f/BIDS/demo_wmh/code/config.yml \\\n  --run_dcm2bids \\\n  --subject_id SUB0001 --session_id 01 \\\n  --dicom_subdir DICOM_01\n</code></pre> <p>Theoretically, the folder <code>/mnt/f/BIDS/demo_wmh/sub-SUB0001</code> should be created to store the subject's data. However, since we did not check the <code>SeriesDescription</code> field of the DICOM files in advance, it should prompt that no matching files were found, and the subject folder was not created. Next, we can open the <code>tmp_dcm2bids</code> folder to check the output of <code>dcm2bids</code>, find the json file of the image of interest, and then find the <code>SeriesDescription</code> field (or other fields you want to match) to modify the corresponding content in <code>dcm2bids_config.json</code>, and run the above command again.</p> <p>At this time, it should be able to successfully obtain the <code>sub-SUB0001</code> folder, which contains the <code>ses-01</code> subfolder. Because <code>dcm2bids</code> will automatically look for images that meet the criteria under <code>tmp_dcm2bids</code>, instead of converting all images again.</p>"},{"location":"dcm2bids/dcm2bids/#bidsignore","title":".bidsignore","text":"<p>We can notice that there is a <code>.bidsignore</code> file in the bids root directory, which is used to ignore files that do not belong to the BIDS format. If we open it, we will find that it contains the <code>tmp_dcm2bids</code> folder, which is the temporary folder generated when we run the dcm2bids command. The <code>dcm2bids</code> will automatically add it to the <code>.bidsignore</code>.</p> <p>It is worth noting that <code>dcm2bids</code> itself is very flexible and allows the generation of folders that do not meet the BIDS requirements (for example, I want to change the datatype and suffix in the json file to qsm and GRE respectively, which I believe is not currently specified in the BIDS specification, but this is more convenient for organizing data). In this case, we need to manually add these folders to the <code>.bidsignore</code> (for example: sub-*/ses-*/qsm/ to ignore each qsm folder). This is very necessary because various nipreps (such as fmriprep, qsiprep) include a check bids validation step (although it can be skipped, it is not recommended to do so, otherwise it is easy to have no error but the running process has problems), and these folders that do not meet the standard definition will cause errors.</p> <p>In addition, the BIDSLayout function of nipype seems to automatically exclude folders that do not meet the BIDS definition, regardless of whether they are added to the <code>.bidsignore</code>, which is also why we did not use it.</p>"},{"location":"pipelines/","title":"Pipelines Overview","text":""},{"location":"pipelines/#structural-mri-smri-pipelines","title":"Structural MRI (sMRI) Pipelines","text":"<ul> <li> Freesurfer recon-all (freesurfer)</li> <li> FSL anat (fsl_anat)</li> <li> WMH Quantification (wmh_quantification)</li> <li> PVS Quantification (pvs_quantification)</li> <li> CMB Quantification (cmb_quantification)</li> </ul>"},{"location":"pipelines/#diffusion-mri-dmri-pipelines","title":"Diffusion MRI (dMRI) Pipelines","text":"<ul> <li> General DWI Processing (dwi_pipeline)</li> <li> Lesion Quantification Toolkit (LQT) Pipeline (lqt_pipeline)</li> </ul>"},{"location":"pipelines/#quantitative-mri-qmri-pipelines","title":"Quantitative MRI (qMRI) Pipelines","text":"<ul> <li> Quantitative Susceptibility Mapping (sepia_qsm)</li> </ul>"},{"location":"pipelines/#dsc-mri-pwi-pipelines","title":"DSC-MRI (PWI) Pipelines","text":"<ul> <li> PWI Pipeline (pwi_pipeline)</li> </ul>"},{"location":"pipelines/dMRI/lqt_pipeline/","title":"LQT Pipeline","text":"<p>Using the Lesion Quantification Toolkit (LQT) to quantify lesion disconnection.</p> <p>Last updated: 2025-08-02, WYJ</p>"},{"location":"pipelines/dMRI/lqt_pipeline/#cvdproc.pipelines.dmri.lqt_pipeline.LQTPipeline.__init__","title":"<code>__init__</code>","text":"<p>LQT Pipeline for lesion disconnection analysis.</p> <p>Parameters:</p> Name Type Description Default <code>subject</code> <code>BIDSSubject</code> <p>A BIDS subject object.</p> required <code>session</code> <code>BIDSSession</code> <p>A BIDS session object.</p> required <code>output_path</code> <code>str</code> <p>Directory to save outputs.</p> required <code>seed_mask</code> <code>str</code> <p>Name of the seed mask folder in 'derivatives'. For example, if the ROI mask path is 'derivatives/lesion_mask/sub-XXX/ses-XXX/*infarction.nii.gz', then seed_mask='lesion_mask'.</p> <code>'lesion_mask'</code> <code>use_which_mask</code> <code>str</code> <p>Keyword to select the desired lesion mask. Default is 'infarction'. For example, if the lesion mask is 'derivatives/lesion_mask/sub-XXX/ses-XXX/*infarction.nii.gz', then use_which_mask='infarction'.</p> <code>'infarction'</code> <code>extract_from</code> <code>str</code> <p>If extracting results, please provide it.</p> <code>None</code>"},{"location":"pipelines/dMRI/lqt_pipeline/#cvdproc.pipelines.dmri.lqt_pipeline.LQTPipeline.check_data_requirements","title":"<code>check_data_requirements</code>","text":"<p>Will always return True, as the LQT pipeline will check the seed mask in MNI space during creation of the workflow.</p>"},{"location":"pipelines/dMRI/lqt_pipeline/#a-more-detailed-description","title":"A more detailed description:","text":"<p>Source: LQT</p> <p>Note</p> <p>LQT is implemented in R, so you need to have R installed on your system. Using R in WSL seems to have some instability, try changing the cores=4 parameter in 'cvdproc/pipelines/r/lqt_single_subject.R'.</p> <p>I recommend using the RStudio IDE (because we use rstudioapi in the script) and open 'cvdproc/pipelines/r/lqt_initialize.r' to initialize the LQT package: As the package is no longer being actively maintained, we included the package in 'cvdproc/pipelines/external/LQT' and we can install it from there. Then we need to copy some extension data to the package directory (data is in 'cvdproc/data/lqt/extdata'), which is done in the script. Doing this by following instructions in the LQT README seems incorrect as it will download a MacOS version of DSI Studio, which is not compatible with WSL Linux.</p>"},{"location":"pipelines/pwi/pwi_pipeline/","title":"PWI Pipeline","text":"<p>Postprocessing pipeline for Dynamic Susceptibility Contrast MRI (DSC-MRI) perfusion data (PWI).</p> <p>This pipeline is to: Calculate PWI maps from DSC-MRI data (rCBF, rCBV, MTT, TTP, K2)</p> <p>Last updated: 2025-07-27, WYJ</p>"},{"location":"pipelines/pwi/pwi_pipeline/#cvdproc.pipelines.pwi.pwi_pipeline.PWIPipeline.__init__","title":"<code>__init__</code>","text":"<p>Postprocessing pipeline for DSC-MRI perfusion data (PWI).</p> <p>Parameters:</p> Name Type Description Default <code>subject</code> <code>BIDSSubject</code> <p>A BIDS subject object.</p> required <code>session</code> <code>BIDSSession</code> <p>A BIDS session object.</p> required <code>output_path</code> <code>str</code> <p>Directory to save outputs.</p> required <code>use_which_pwi</code> <code>str</code> <p>Keyword to select the desired PWI file.</p> <code>'pwi'</code> <code>dsc_mri_toolbox_path</code> <code>str</code> <p>Path to the MATLAB dsc-mri-toolbox.</p> <code>None</code> <code>extract_from</code> <code>str</code> <p>If extracting results, please provide it.</p> <code>None</code>"},{"location":"pipelines/pwi/pwi_pipeline/#a-more-detailed-description","title":"A more detailed description:","text":"<ol> <li> <p>Strip the PWI data using SynthStrip to create a brain mask.</p> </li> <li> <p>Calculate the PWI concentration map (a 4D image).</p> </li> <li> <p>Auto select the AIF (arterial input function) from the concentration data. (Please refer to Auto AIF Selection)</p> </li> <li> <p>Generate the perfusion maps (rCBF, rCBV, MTT, TTP, K2) using the MATLAB-based dsc-mri-toolbox.</p> </li> </ol>"},{"location":"pipelines/qMRI/sepia_qsm/","title":"Sepia QSM","text":"<p>Pipeline for processing Quantitative Susceptibility Mapping (QSM) data using the SEPIA toolbox.</p>"},{"location":"pipelines/qMRI/sepia_qsm/#cvdproc.pipelines.qmri.sepia_qsm_pipeline.SepiaQSMPipeline.__init__","title":"<code>__init__</code>","text":"<p>Sepia QSM processing pipeline</p> <p>Parameters:</p> Name Type Description Default <code>subject</code> <code>BIDSSubject</code> <p>A BIDS subject object.</p> required <code>session</code> <code>BIDSSession</code> <p>A BIDS session object.</p> required <code>output_path</code> <code>str</code> <p>Output directory to save results.</p> required <code>use_which_t1w</code> <code>str</code> <p>Keyword to select the desired T1w image.</p> <code>None</code> <code>normalize</code> <code>bool</code> <p>If True, normalize QSM (and other scalar maps) to MNI space via T1w.</p> <code>False</code> <code>sepia_toolbox_path</code> <code>str</code> <p>Path to the SEPIA toolbox. If None, assumes SEPIA is in MATLAB path.</p> <code>None</code> <code>reverse_phase</code> <code>int</code> <p>Set to 1 to reverse phase polarity (for GE scanners).</p> <code>0</code>"},{"location":"pipelines/qMRI/sepia_qsm/#a-more-detailed-description","title":"A more detailed description:","text":""},{"location":"pipelines/sMRI/pvs_quantification/","title":"pvs_quantification","text":"<p>Current SHiVAi pipeline (2025-06) seems to have problem handling html/pdf reports (for example, when facing NaN value). A workaround is to modify the code in <code>post.py</code> to simply skip the report generation step, which is not critical for the pipeline to run.</p> <pre><code>class SummaryReport(BaseInterface):\n    \"\"\"Make a summary report of preprocessing and prediction\"\"\"\n    input_spec = SummaryReportInputSpec\n    output_spec = SummaryReportOutputSpec\n\n    # def _run_interface(self, runtime):\n    #     \"\"\"\n    #     Build the report for the whole workflow. It contains segmentation statistics and\n    #     quality control figures.\n\n    #     \"\"\"\n    #     if self.inputs.anonymized:\n    #         subject_id = ''\n    #     else:\n    #         subject_id = self.inputs.subject_id\n\n    #     brain_vol_vox = nib.load(self.inputs.brainmask).get_fdata().astype(bool).sum()  # in voxels\n    #     pred_metrics_dict = {}  # Will contain the stats dataframe for each biomarker\n    #     pred_census_im_dict = {}  # Will contain the path to the swarm plot for each biomarker\n    #     pred_overlay_im_dict = {}  # Will contain the path to the figure with biomarkers overlaid on the brain\n    #     models_uid = {}  # Will contain the md5 hash for each file of each predictive model\n    #     pred_and_acq = self.inputs.pred_and_acq\n\n    #     # Generate the distribution figures for each prediction and fill models_uid\n    #     for pred in pred_and_acq:\n    #         lpred = pred.lower()  # \"pred\" is uppercase, so we also need a lowercase version\n    #         if pred == 'LAC':\n    #             name_in_plot = 'Lacuna'\n    #         else:\n    #             name_in_plot = pred\n    #         models_uid[pred] = {}\n    #         pred_metrics_dict[pred] = pd.read_csv(getattr(self.inputs, f'{lpred}_metrics_csv'))\n    #         if pred_metrics_dict[pred]['Number of clusters'].sum() == 0:  # No biomarker detected\n    #             pred_census_im_dict[pred] = None\n    #         else:\n    #             pred_census_im_dict[pred] = violinplot_from_census(getattr(self.inputs, f'{lpred}_census_csv'),\n    #                                                                self.inputs.resolution,\n    #                                                                name_in_plot)\n    #         pred_overlay_im_dict[pred] = getattr(self.inputs, f'{lpred}_overlay')\n    #         ids, url = get_md5_from_json(getattr(self.inputs, f'{lpred}_model_descriptor'), get_url=True)\n    #         models_uid[pred]['id'] = ids\n    #         if url:\n    #             models_uid[pred]['url'] = url\n\n    #     # set optional inputs to None if undefined\n    #     if isdefined(self.inputs.overlayed_brainmask_1):\n    #         overlayed_brainmask_1 = self.inputs.overlayed_brainmask_1\n    #     else:\n    #         overlayed_brainmask_1 = None\n    #     if isdefined(self.inputs.crop_brain_img):\n    #         crop_brain_img = self.inputs.crop_brain_img\n    #     else:\n    #         crop_brain_img = None\n    #     if isdefined(self.inputs.isocontour_slides_FLAIR_T1):\n    #         isocontour_slides_FLAIR_T1 = self.inputs.isocontour_slides_FLAIR_T1\n    #     else:\n    #         isocontour_slides_FLAIR_T1 = None\n    #     if isdefined(self.inputs.overlayed_brainmask_2):\n    #         overlayed_brainmask_2 = self.inputs.overlayed_brainmask_2\n    #     else:\n    #         overlayed_brainmask_2 = None\n    #     if isdefined(self.inputs.wf_graph):\n    #         wf_graph = self.inputs.wf_graph\n    #     else:\n    #         wf_graph = None\n    #     if isdefined(self.inputs.db):\n    #         db = self.inputs.db\n    #     else:\n    #         db = ''\n    #     # process\n    #     html_report = make_report(\n    #         pred_metrics_dict=pred_metrics_dict,\n    #         pred_census_im_dict=pred_census_im_dict,\n    #         pred_overlay_im_dict=pred_overlay_im_dict,\n    #         pred_and_acq=pred_and_acq,\n    #         brain_vol_vox=brain_vol_vox,\n    #         thr_cluster_vals=self.inputs.thr_cluster_vals,\n    #         min_seg_size=self.inputs.min_seg_size,\n    #         models_uid=models_uid,\n    #         bounding_crop=crop_brain_img,\n    #         overlayed_brainmask_1=overlayed_brainmask_1,\n    #         overlayed_brainmask_2=overlayed_brainmask_2,\n    #         isocontour_slides_FLAIR_T1=isocontour_slides_FLAIR_T1,\n    #         subject_id=subject_id,\n    #         image_size=self.inputs.image_size,\n    #         resolution=self.inputs.resolution,\n    #         percentile=self.inputs.percentile,\n    #         threshold=self.inputs.threshold,\n    #         wf_graph=wf_graph\n    #     )\n\n    #     with open('Shiva_report.html', 'w', encoding='utf-8') as fid:\n    #         fid.write(html_report)\n\n    #     # Convert the HTML file to PDF using CSS\n    #     # Creating custom CSSin addition to the main one for the pages header and the logos\n    #     postproc_dir = os.path.dirname(postproc_init)\n    #     css = CSS(os.path.join(postproc_dir, 'printed_styling.css'))\n    #     now = datetime.now(timezone.utc).strftime(\"%Y/%m/%d - %H:%M (UTC)\")\n    #     content_sub_id = f'Patient ID: {subject_id} \\A ' if subject_id else ''\n    #     header = (\n    #         '@page {'\n    #         '   @top-left {'\n    #         f'      content: \"{content_sub_id}Data-base: {db}\";'\n    #         '       font-size: 10pt;'\n    #         '       white-space: pre;'\n    #         '   }'\n    #         '   @top-center {'\n    #         f'      content: \"{now}\";'\n    #         '       font-size: 10pt;'\n    #         '   }'\n    #         '}'\n    #     )\n    #     css_header = CSS(string=header)\n    #     shiva_logo_file = os.path.join(postproc_dir, 'logo_shiva.png')\n    #     other_logos_file = os.path.join(postproc_dir, 'logos_for_shiva.png')\n    #     with open(shiva_logo_file, 'rb') as f:\n    #         image_data = f.read()\n    #         shiva_logo = base64.b64encode(image_data).decode()\n    #     with open(other_logos_file, 'rb') as f:\n    #         image_data = f.read()\n    #         other_logo = base64.b64encode(image_data).decode()\n    #     logo = (\n    #         '@page {'\n    #         '   @bottom-left {'\n    #         f'      background-image: url(data:image/png;base64,{other_logo});'\n    #         '       background-size: 552px 45px;'\n    #         '       display: inline-block;'\n    #         '       width: 560px; '\n    #         '       height: 60px;'\n    #         '       content:\"\";'\n    #         '       background-repeat: no-repeat;'\n    #         '   }'\n    #         '   @top-right-corner {'\n    #         f'      background-image: url(data:image/png;base64,{shiva_logo});'\n    #         '       background-size: 70px 70px;'\n    #         '       display: inline-block;'\n    #         '       width: 70px; '\n    #         '       height: 70px;'\n    #         '       content:\"\";'\n    #         '       background-repeat: no-repeat;'\n    #         '   }'\n    #         '}'\n    #     )\n    #     logo_css = CSS(string=logo)\n    #     HTML('Shiva_report.html').write_pdf('Shiva_report.pdf',\n    #                                         stylesheets=[css, css_header, logo_css])\n\n    #     setattr(self, 'html_report', os.path.abspath('Shiva_report.html'))\n    #     setattr(self, 'pdf_report', os.path.abspath('Shiva_report.pdf'))\n    #     return runtime\n\n    def _run_interface(self, runtime):\n        \"\"\"Skip actual report generation but create placeholder files.\"\"\"\n        print(\"SummaryReport disabled: generating empty HTML and PDF files to avoid workflow errors.\")\n\n        # Create empty HTML file\n        html_path = os.path.abspath('Shiva_report.html')\n        with open(html_path, 'w', encoding='utf-8') as f:\n            f.write('&lt;html&gt;&lt;body&gt;&lt;p&gt;Summary report generation skipped.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;')\n\n        # Create empty (but valid) PDF file header to avoid downstream rendering issues\n        pdf_path = os.path.abspath('Shiva_report.pdf')\n        with open(pdf_path, 'wb') as f:\n            f.write(b'%PDF-1.4\\n%EOF\\n')  # minimal valid PDF structure\n\n        setattr(self, 'html_report', html_path)\n        setattr(self, 'pdf_report', pdf_path)\n        return runtime\n\n\n    def _list_outputs(self):\n        \"\"\"Fill in the output structure.\"\"\"\n        outputs = self.output_spec().trait_get()\n        outputs['html_report'] = getattr(self, 'html_report')\n        outputs['pdf_report'] = getattr(self, 'pdf_report')\n\n        return outputs\n</code></pre>"},{"location":"pipelines/sMRI/wmh_quantification/","title":"WMH Quantification","text":"<p>pipeline name: <code>wmh_quantification</code></p>"}]}